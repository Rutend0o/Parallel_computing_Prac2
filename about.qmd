---
title: " "
---


```{r}
#| include: false
#| message: false
library (foreach)
library(doParallel)
library(MASS)
library(foreach)
library(iterators)
library(parallel)
```




# Question 1 
Random samples from Exponential Distribution using foreach: Mean and variance 
```{r}
set.seed(1)
results<-foreach (i= 1:100, .combine = rbind)%do%{
  sample_data<-rexp(100, rate =1)
  sample_mean<- mean(sample_data)
  sample_variance<- var(sample_data)
  c(sample_mean,sample_variance)
}
rownames(results) <- NULL # Remove row names
head(results)
```
<hr style="border: 2px solid #2196F3;">  <!-- Custom Blue Divider -->

# Question 2 
Comparison of parallel vs serial bootstrapping for estimating the median of the Galaxies dataset

```{r}
#| message: false
set.seed(1)
data(galaxies)
B=10000
#Bootstrap iteration serial processing 
boot_median<-numeric(B)
BS_median<- function (data, B){
  for (i in 1:B){
    BS_samples<- sample(data, replace = TRUE)
    boot_median[i]<- median (BS_samples)
  }
  return(boot_median)
}
#serial processing

system.time({
  serial_boot<-BS_median(galaxies,10000)
})

#parallel processing 
cores<- detectCores()

cl<-makeCluster(4-1)
registerDoParallel(cl)
system.time({
  parallel_boot<- foreach(i=1:10000, .combine=c, .packages="MASS")%dopar% {
  median(sample(galaxies, replace=TRUE))}
})
stopCluster(cl)
#batch parallel computing
cores <- detectCores()
cl <- makeCluster(4 - 1)
registerDoParallel(cl)
# Each worker computes 1000
system.time({
  chunk_parallel_boot <- foreach(i = 1:10, .combine = c, .packages = "MASS") %dopar% {
    replicate(1000, median(sample(galaxies, replace = TRUE)))
  }
})

stopCluster(cl)
```
Based on the results, the runtime for serial processing is low (1.08 seconds) but it still took longer compared to batch parallel processing. Serial processing works well for smaller datasets or tasks where the overhead of parallel processing might outweigh its benefits. The time for parallel processing is the highest (6.61 seconds), suggesting that for this task and dataset size parallel computing didn't  speed up the processing.The overhead of managing multiple cores and data transfer between them likely outweighed the benefits of parallel computing, especially since each iteration in bootstrapping is relatively small and lightweight.The batch parallel processing (processing 1000 bootstrap samples at a time) is faster than the other two. it has a much lower total runtime (0.61 seconds). This indicates that grouping multiple iterations into larger batches for parallel computing helps to minimize the overhead, making it more efficient. 

<hr style="border: 2px solid #2196F3;">  <!-- Custom Blue Divider -->


# Question 3 
Estimate coverage of a percentile bootstrap confidence interval
```{r}
closeAllConnections()
set.seed(1)  
n <- 50      
B <- 1000  # Bootstrap samples
c_n <- 1000  
  
coverage  <- 0
# Simulation loop to estimate coverage probability
for (i in 1:c_n) {
  q3data <- rexp(n, rate = 1)  
  # Bootstrap CI calculation 
  boot_means <- replicate(B, mean(sample(q3data, replace = TRUE)))
  lowerCI <- quantile(boot_means, prob=0.025)
  upperCI <- quantile(boot_means, prob=0.975)
  
  # Check if true mean (1) is within the CI
  if (1 >= lowerCI && 1 <= upperCI) {
    coverage <- coverage + 1
  }
}  

# Coverage Probability
coverage_prob <- coverage/ c_n
cat("coverage\n", coverage_prob )

```
<hr style="border: 2px solid #2196F3;">  <!-- Custom Blue Divider -->


# Question 4 
Finding the maximum Values in Random Vectors Using foreach and irnorm
```{r}
set.seed(1234)
irnorm <- function(n) rnorm(n)
q4data  <- lapply(1:3, function(x) irnorm(5))
max_values <- foreach(i = q4data, .combine = c) %do% {
  max(i) 
}
print(max_values)

```
<hr style="border: 2px solid #2196F3;">  <!-- Custom Blue Divider -->


# Question 5 
Comparing run time between parLapply, foreach and replicate
```{r}
#| message: false
closeAllConnections()
irnorm <- function(n) rnorm(n)
set.seed(1234)
q4data <- lapply(1:3, function(x) irnorm(5))

#ParLapply
cl <- makeCluster(detectCores() - 1)  
clusterExport(cl, list("irnorm"))  
invisible(clusterEvalQ(cl, library(iterators)))  
system.time({
  max_values_parLapply <- parLapply(cl, q4data, max)  
})
stopCluster(cl)

#Foreach method
set.seed(1234)
system.time({
  max_values_foreach <- foreach(v = q4data, .combine = c) %do% {
    max(v)  
  }
})

#replicate method
set.seed(1234)
system.time({
  max_values_replicate <- replicate(3, max(irnorm(5))) 
})
max_values_foreach
max_values_replicate


```
All methods have quick runtimes for this problem.For small datasets, the parallel execution times (parLapply, foreach) may not show significant benefits over a serial methods eg replicate. However as the problem becomes big (more iterations, more random values), the efficiency of parallel processing becomes more evident. In such cases, their runtime would decrease compared to serial processing
